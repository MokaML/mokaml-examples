{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "# Polyaxon\n",
    "from polyaxon_client.tracking import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATIONS = {\n",
    "    'relu': tf.nn.relu,\n",
    "    'sigmoid': tf.sigmoid,\n",
    "    'tanh': tf.tanh,\n",
    "}\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    'gradient_descent': tf.train.GradientDescentOptimizer,\n",
    "    'rmsprop': tf.train.RMSPropOptimizer,\n",
    "    'adam': tf.train.AdamOptimizer,\n",
    "}\n",
    "\n",
    "MNIST_HOST = 'http://yann.lecun.com/exdb/mnist/'\n",
    "TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "IMAGE_WIDTH = 28\n",
    "OUTPUT_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_onehot_data(filename):\n",
    "    with gzip.open(filename, 'rb') as unzipped_file:\n",
    "        data = np.frombuffer(unzipped_file.read(), dtype=np.uint8)\n",
    "    labels = data[8:]\n",
    "    length = len(labels)\n",
    "    onehot = np.zeros((length, OUTPUT_CLASSES), dtype=np.float32)\n",
    "    onehot[np.arange(length), labels] = 1\n",
    "    return onehot\n",
    "\n",
    "\n",
    "def load_image_data(filename):\n",
    "    with gzip.open(filename, 'rb') as unzipped_file:\n",
    "        data = np.frombuffer(unzipped_file.read(), dtype=np.uint8)\n",
    "    images = data[16:].reshape((-1, IMAGE_WIDTH ** 2)).astype(np.float32)\n",
    "    images /= 255\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_mnist_data(path='/tmp/mnist'):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    for data_file in [\n",
    "        TRAIN_IMAGES,\n",
    "        TRAIN_LABELS,\n",
    "        TEST_IMAGES,\n",
    "        TEST_LABELS,\n",
    "    ]:\n",
    "        destination = os.path.join(path, data_file)\n",
    "        if not os.path.isfile(destination):\n",
    "            urlretrieve(\"{}{}\".format(MNIST_HOST, data_file), destination)\n",
    "    return (\n",
    "        (load_image_data(os.path.join(path, TRAIN_IMAGES)),\n",
    "         load_onehot_data(os.path.join(path, TRAIN_LABELS))),\n",
    "        (load_image_data(os.path.join(path, TEST_IMAGES)),\n",
    "         load_onehot_data(os.path.join(path, TEST_LABELS))),\n",
    "    )\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "\n",
    "def conv_layer(x, filter_size, out_features, activation, pool_size):\n",
    "    W = weight_variable([filter_size, filter_size, x.get_shape()[3].value, out_features])\n",
    "    b = bias_variable([out_features])\n",
    "    conv = ACTIVATIONS[activation](tf.nn.conv2d(x, W, [1, 1, 1, 1], padding='SAME') + b)\n",
    "    pool = tf.nn.max_pool(conv, ksize=[1, pool_size, pool_size, 1],\n",
    "                          strides=[1, pool_size, pool_size, 1], padding='SAME')\n",
    "    return pool\n",
    "\n",
    "\n",
    "def fully_connected_layer(x, out_size):\n",
    "    W = weight_variable([x.get_shape()[1].value, out_size])\n",
    "    b = bias_variable([out_size])\n",
    "    return tf.matmul(x, W) + b\n",
    "\n",
    "\n",
    "def create_model(conv1_size,\n",
    "                 conv1_out,\n",
    "                 conv1_activation,\n",
    "                 pool1_size,\n",
    "                 conv2_size,\n",
    "                 conv2_out,\n",
    "                 conv2_activation,\n",
    "                 pool2_size,\n",
    "                 fc1_activation,\n",
    "                 fc1_size,\n",
    "                 optimizer,\n",
    "                 log_learning_rate):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, IMAGE_WIDTH ** 2])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, OUTPUT_CLASSES])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    input_image = tf.reshape(x, [-1, IMAGE_WIDTH, IMAGE_WIDTH, 1])\n",
    "\n",
    "    conv1 = conv_layer(input_image, conv1_size, conv1_out, conv1_activation, pool1_size)\n",
    "\n",
    "    conv2 = conv_layer(conv1, conv2_size, conv2_out, conv2_activation, pool2_size)\n",
    "\n",
    "    _, conv2_height, conv2_width, conv2_features = conv2.get_shape()\n",
    "    flattened = tf.reshape(conv2,\n",
    "                           [-1, conv2_height.value * conv2_width.value * conv2_features.value])\n",
    "\n",
    "    fc_1 = ACTIVATIONS[fc1_activation](fully_connected_layer(flattened, fc1_size))\n",
    "    fc_1_drop = tf.nn.dropout(fc_1, keep_prob)\n",
    "\n",
    "    y_conv = fully_connected_layer(fc_1_drop, OUTPUT_CLASSES)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "    train_step = OPTIMIZERS[optimizer](10 ** log_learning_rate).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return x, y, y_conv, keep_prob, train_step, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, x_train, y_train, batch_size, dropout, epochs):\n",
    "    x, y, y_conv, keep_prob, train_step, _ = model\n",
    "    train_length = len(x_train)\n",
    "    for i in range(epochs):\n",
    "        indices = np.arange(train_length)\n",
    "        np.random.shuffle(indices)\n",
    "        for start in range(0, train_length, batch_size):\n",
    "            end = min(start + batch_size, train_length)\n",
    "            batch_indices = indices[start:end]\n",
    "            x_batch, y_batch = x_train[batch_indices], y_train[batch_indices]\n",
    "            train_step.run(feed_dict={x: x_batch, y: y_batch, keep_prob: dropout})\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    x, y, y_conv, keep_prob, _, accuracy = model\n",
    "    return accuracy.eval(feed_dict={x: x_test, y: y_test, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_size_param=5\n",
    "conv1_out_param=32\n",
    "conv1_activation_param='relu'\n",
    "pool1_size_param=2\n",
    "conv2_size_param=5\n",
    "conv2_out_param=64\n",
    "conv2_activation_param='relu'\n",
    "pool2_size_param=2\n",
    "fc1_activation_param='sigmoid'\n",
    "fc1_size_param=1024\n",
    "optimizer_param='adam'\n",
    "log_learning_rate_param=-3\n",
    "batch_size_param=100\n",
    "dropout_param=0.2\n",
    "epochs_param=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polyaxon\n",
    "experiment = Experiment('mnist')\n",
    "experiment.create(framework='tensorflow', tags=['examples'])\n",
    "experiment.log_params(\n",
    "    conv1_size=conv1_size_param,\n",
    "    conv1_out=conv1_out_param,\n",
    "    conv1_activation=conv1_activation_param,\n",
    "    pool1_size=pool1_size_param,\n",
    "    conv2_size=conv2_size_param,\n",
    "    conv2_out=conv2_out_param,\n",
    "    conv2_activation=conv2_activation_param,\n",
    "    pool2_size=pool2_size_param,\n",
    "    fc1_activation=fc1_activation_param,\n",
    "    fc1_size=fc1_size_param,\n",
    "    optimizer=optimizer_param,\n",
    "    log_learning_rate=log_learning_rate_param,\n",
    "    batch_size=batch_size_param,\n",
    "    dropout=dropout_param,\n",
    "    epochs=epochs_param)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist_data()\n",
    "\n",
    "# Polyaxon\n",
    "experiment.log_data_ref(data=x_train, data_name='x_train')\n",
    "experiment.log_data_ref(data=y_train, data_name='y_train')\n",
    "experiment.log_data_ref(data=x_test, data_name='x_test')\n",
    "experiment.log_data_ref(data=y_test, data_name='y_test')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = create_model(\n",
    "        conv1_size=conv1_size_param,\n",
    "        conv1_out=conv1_out_param,\n",
    "        conv1_activation=conv1_activation_param,\n",
    "        pool1_size=pool1_size_param,\n",
    "        conv2_size=conv2_size_param,\n",
    "        conv2_out=conv2_out_param,\n",
    "        conv2_activation=conv2_activation_param,\n",
    "        pool2_size=pool2_size_param,\n",
    "        fc1_activation=fc1_activation_param,\n",
    "        fc1_size=fc1_size_param,\n",
    "        optimizer=optimizer_param,\n",
    "        log_learning_rate=log_learning_rate_param)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_model(model,\n",
    "                x_train,\n",
    "                y_train,\n",
    "                batch_size=batch_size_param,\n",
    "                dropout=dropout_param,\n",
    "                epochs=epochs_param)\n",
    "    accuracy = evaluate_model(model, x_test, y_test)\n",
    "\n",
    "    # Polyaxon\n",
    "    experiment.log_metrics(accuracy=accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
