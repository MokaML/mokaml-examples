{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train error vs Test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustration of how the performance of an estimator on unseen data (test data)\n",
    "is not the same as the performance on training data. As the regularization\n",
    "increases the performance on train decreases while the performance on test\n",
    "is optimal within a range of values of the regularization parameter.\n",
    "The example with an Elastic-Net regression model and the performance is\n",
    "measured using the explained variance a.k.a. R^2.\n",
    "\n",
    "Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "\n",
    "License: BSD 3 clause\n",
    "\n",
    "Source : https://github.com/scikit-learn/scikit-learn/blob/master/examples/model_selection/plot_train_error_vs_test_error.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import polyaxon package*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyaxon_client.tracking import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generate sample data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_train, n_samples_test, n_features = 75, 150, 500\n",
    "np.random.seed(0)\n",
    "coef = np.random.randn(n_features)\n",
    "coef[50:] = 0.0  # only the top 10 features are impacting the model\n",
    "X = np.random.randn(n_samples_train + n_samples_test, n_features)\n",
    "y = np.dot(X, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split train and test data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:n_samples_train], X[n_samples_train:]\n",
    "y_train, y_test = y[:n_samples_train], y[n_samples_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compute train and test errors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-5, 1, 60)\n",
    "\n",
    "enet = linear_model.ElasticNet(l1_ratio=0.7, max_iter=10000)\n",
    "train_errors = list()\n",
    "test_errors = list()\n",
    "for alpha in alphas:\n",
    "    experiment = Experiment()\n",
    "    experiment.create()\n",
    "    enet.set_params(alpha=alpha)\n",
    "    enet.fit(X_train, y_train)\n",
    "    train_errors.append(enet.score(X_train, y_train))\n",
    "    test_errors.append(enet.score(X_test, y_test))\n",
    "    #Track parameters and metrics, the result can me exploited in the UI.\n",
    "    experiment.log_metrics(train_errors = enet.score(X_train, y_train))\n",
    "    experiment.log_metrics(test_errors = enet.score(X_test, y_test))\n",
    "    experiment.log_params(l1_ratio = 0.7, max_iter = 10000, alpha = alpha)\n",
    "\n",
    "i_alpha_optim = np.argmax(test_errors)\n",
    "alpha_optim = alphas[i_alpha_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % alpha_optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Estimate the coef_ on full data with optimal regularization parameter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet.set_params(alpha=alpha_optim)\n",
    "coef_ = enet.fit(X, y).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot results functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.semilogx(alphas, train_errors, label='Train')\n",
    "plt.semilogx(alphas, test_errors, label='Test')\n",
    "plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([0, 1.2])\n",
    "plt.xlabel('Regularization parameter')\n",
    "plt.ylabel('Performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Show estimated coef_ vs true coef*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(coef, label='True coef')\n",
    "plt.plot(coef_, label='Estimated coef')\n",
    "plt.legend()\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
